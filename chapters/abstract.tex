This thesis presents three models based on Generative Stochastic Networks (GSN) for sequence prediction. GSNs are a recent probabilistic generalization of denoising auto-encoders that learn unsupervised hierarchical representations of complex input data, while being trainable by backpropagation.

The first model, the Temporal GSN (TGSN), uses the latent state variables \(H\) learned by the GSN to reduce input complexity such that learning \(H\) over time becomes linear. This means a simple linear regression step \(H \rightarrow H\) can encode the next set of latent state variables describing the input data in the sequence, learning \(P(H_{t+1}|H_{t-m}^t)\) for an arbitrary window of size \(m\).

The second model, the Recurrent GSN (RNN-GSN), is an online model that views GSNs as recurrent networks that encode sequences of input over time. These networks learn features \(H\) of the input data that best predict the next expected data in the sequence. Similarly to other RNN's, the RNN-GSN can be enhanced Hessian-free optimization.

The third model, the Recurrent Temporal GSN (RT-GSN), uses a hybrid approach of combining an input reconstruction GSN layer with an RNN-GSN layer, allowing the GSN to learn useful features of the input data while the recurrent network learns the sequence representation over these features.

Experimental results for these three models are presented on sequential handwritten digit (MNIST) data, and samples generated from the models are compared. The main contribution of this thesis is to provide evidence that GSNs are a viable framework to learn useful representations of complex sequential input data, and to suggest a new framework for deep generative models to learn complex sequences over representations.