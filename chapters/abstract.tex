This thesis presents three models based on Generative Stochastic Networks (GSN) for unsupervised sequence learning and prediction. GSNs are a recent probabilistic generalization of denoising auto-encoders that learn unsupervised hierarchical representations of complex input data, while being trainable by backpropagation.

The first model, the Temporal GSN (TGSN), uses the latent state variables \(H\) learned by the GSN to reduce input complexity such that learning the representations \(H\) over time becomes linear. This means a simple linear regression step \(H \rightarrow H\) can encode the next set of latent state variables describing the input data in the sequence, learning \(P(H_{t+1}|H_{t-m}^t)\) for an arbitrary history, or context, window of size \(m\).

The second model, the Recurrent GSN (RNN-GSN), is an online model that uses a Recurrent Neural Network (RNN) to learn the sequences of GSN parameters \(H\) over time. By having the progression of \(H\) learned by an RNN instead of through regression like the TGSN, this model can learn sequences with arbitrary time dependencies. Similarly to other RNN's, the RNN-GSN can be enhanced using Hessian-free optimization or LTSM nodes.

The third model, the Sequence Encoding Network (SEN), is a novel framework for learning deep sequence representations. It uses a hybrid approach of combining a reconstruction generative network layers with recurrent layers, allowing the model to learn a deep representation of complex time dependencies.

Experimental results for these three models are presented on sequential handwritten digit (MNIST) data, and samples generated from the models are compared. The main contribution of this thesis is to provide evidence that GSNs are a viable framework to learn useful representations of complex sequential input data, and to suggest a new framework for deep generative models to learn complex sequences by decoupling static input representations from dynamic time dependency representations.