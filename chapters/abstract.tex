This thesis presents three methods of using Generative Stochastic Networks (GSN) for sequence prediction. GSNs are a recent probabilistic generalization of denoising auto-encoders that learn unsupervised deep hierarchical representations of complex input data, while being trainable by backpropagation. Each method presented relies on learning the transition operator on a Markov chain of the input data over time. The first method (Model 1) views GSNs as learning complex representations of the individual input data using latent state variables \(H\), so that a simple regression step \(H \rightarrow H\) can encode the next set of latent state variables describing the next input data in the sequence, learning \(P(X_{t+1}|X_t, H_t)\). This method is similar to Expectation Maximization (EM), where first a GSN is trained on all input data and then a regression is trained on the sequenced latent states \(H\), which is repeated until training converges. The second method (Model 2) is an online training method that views GSNs as recurrent networks that encode sequences of input over time. This means the networks learn features of the input data that best predict the next expected data in the sequence. The third method (Model 3) uses a hybrid approach of combining an input reconstruction GSN layer with a sequential reconstruction GSN layer, allowing the GSN to learn useful features of the input data while the sequential network learns the sequence representation over these features. Experimental results for these three methods are presented on artificially sequenced handwritten digit MNIST data (digits sequenced 0-9 repeating), and samples generated from the models are compared. The main contribution of this thesis is to provide evidence that GSNs are a viable framework to learn useful representations of complex sequential input data, and to suggest a framework for deep generative models to learn complex sequences over representations as well.