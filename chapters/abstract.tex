This thesis presents three methods of using Generative Stochastic Networks (GSN) for sequence prediction. GSNs are a recent probabilistic generalization of denoising auto-encoders that learn unsupervised deep hierarchical representations of complex input data, while being trainable by backpropagation. Each method presented relies on learning the transition operator on a Markov chain of the input data over time. The first method (Model 1) views GSNs as learning complex representations of the individual input data using latent state variables \(H\), so that a simple regression step \(H \rightarrow H\) can encode the next set of latent state variables describing the next input data in the sequence, learning \(P(X_{t+1}|X_t, H_t)\). This method is similar to Expectation Maximization (EM), where first a GSN is trained on all input data, and then a regression is trained on the sequenced latent states \(H\), which is repeated until training converges. The second method (Model 2) is an online training method that views GSNs as recurrent networks that encode sequences of input over time. This means the networks learn features of the input data that best predict the next expected data in the sequence. The third method (Model 3) uses a hybrid approach of combining GSN layers with a recurrent layer. This allows the GSN to learn useful features of the input data while the recurrent network learns the sequence representation over these features. Experimental results for these three methods are presented on artificially sequenced handwritten digit MNIST data (digits sequenced 0-9 repeating) and data generated from the models.