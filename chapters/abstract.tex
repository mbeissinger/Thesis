This thesis presents two methods of using Generative Stochastic Networks (GSN) for sequence prediction. GSNs are a recent probabilistic generalization of denoising auto-encoders that learn unsupervised deep hierarchical representations of complex input data, while being trainable by backpropagation. Both methods presented rely on learning the transition operator on a Markov chain of the input data over time. The first method (Model 1) views GSNs as learning complex representations of the individual input data using latent state variables \(H\), so that a simple regression step \(H \rightarrow H\) can encode the next set of latent state variables describing the next input data in the sequence, learning \(P(X_{t+1}|X_t, H_t)\). This method is similar to Expectation Maximization (EM), where first a GSN is trained on all input data, and then a regression is trained on the sequenced latent states \(H\), which is repeated until training converges. The second method (Model 2) is an online training method that views GSNs as recurrent networks that encode sequences of input over time. This means the networks learn features of the input data that best predict the next expected data in the sequence. Experimental results for these two methods are presented on artificially sequenced handwritten digit MNIST data and sequenced handwritten form NIST data.