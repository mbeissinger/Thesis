This thesis presents three models using GSNs to learn useful representations of complex input data sequences. It corroborates that deep architectures, such as the related work with RBMs, are extremely powerful ways to learn complex sequences, and that GSNs are an equally viable framework that improve upon training and inference of RBMs. Deep architectures derive most of their power from being able to disentangle the underlying factors of variation in the input data - flattening the data manifolds at higher representations to improve mixing between the many modes.

The Temporal GSN, an EM approach, takes advantage of the GSN's ability to reduce the complexity of the input data at higher layer of representation, allowing for simple linear regression to learn sequences of representations over time. This model learns to reconstruct both the current input and the next predicted input. This reconstructed predicted input tends to look like an average of the next inputs in the sequence given the current input.

The Recurrent GSN adds a recurrent hidden state to learn a sequential representation between the GSN's latent spaces. This approach allows for more complex time series interactions to be learned over the TGSN.

The Sequence Encoder Network generalizes the idea behind the Recurrent GSN. By alternating layers of encoder-decoder models that learn reconstructions of the input, and recurrent layers that learn reconstruction of future prediction, it models hierarchical representations of both the input and sequence spaces. Training is much more difficult as layer numbers increase.
