This thesis presents three models of using GSNs to learn useful representations of complex input data sequences. It corroborates that deep architectures, such as the related work with RBMs, are extremely powerful ways to learn complex sequences, and that GSNs are an equally viable framework that improve upon training and inference of RBMs. Deep architectures derive most of their power from being able to disentangle the underlying factors of variation in the input data - flattening the data manifolds at higher representations to improve mixing between the many modes.

The first model presented, an EM approach, takes advantage of the GSN's ability to reduce the complexity of the input data at higher layer of representation, allowing for simple linear regression to learn sequences of representations over time. This model learns to reconstruct both the current input and the next predicted input. This reconstructed predicted input tends to look like an average of the next inputs in the sequence given the current input.

The second model presented, an online learning approach, uses the hidden representation layers to learn information about the input sequences at the cost of training speed (due to no walkbacks and untied weights between layers) and mixing between modes in the input space. Instead, it learns to mix between modes of the sequence space, encoding the next most likely input given the previous inputs.

The last model presented, the hybrid approach, tries to combine the best of both worlds. By alternating layers of GSNs that learn reconstruction of the input and reconstruction of a prediction, it forms useful representations of both the input and sequence spaces. Training is much more difficult as layer numbers increase, but it has the ability to form useful representations of complex inputs that have complex sequences.