Deep learning research has grown in popularity due to its ability to form useful feature representations of highly complex input data. Useful representations are those that disentangle the factors of variation of input data, preserving the information that is ultimately useful for the given machine learning task. In practice, these representations are used as input features to other algorithms, where in the past features would have been constructed by hand. Deep learning frameworks (especially deep convolutional neural networks \cite{lenet5}) have had recent successes for supervised learning of representations for many tasks, creating breakthroughs for both speech and object recognition \cite{seide11, krizhevsky12}.

Unsupervised learning of representations, however, has had slower progress. These models, mostly Restricted Boltzmann Machines (RBM) \cite{hinton06}, auto-encoders \cite{alain12}, and sparse-coding variants \cite{ranzato07}, suffer from the difficulty of marginalizing across an intractable number of configurations of random variables (observed, latent, or both). Each plausible configuration of latent and observed variables would be a mode in the distribution of interest \(P(X,H)\) or \(P(X)\) directly, and current methods of inference or sampling are forced to make strong assumptions about these distributions. Recent advances on the generative view of denoising auto-encoders and generative stochastic networks \cite{gsn} have alleviated this difficulty by simply learning a local Markov chain transition operator through backpropagation, which is often unimodal (instead of parameterizing the data distribution directly, which is multi-modal). This approach has opened up unsupervised learning of deep representations for many useful tasks, including sequence prediction. Unsupervised sequence prediction and labeling remains an important problem for artificial intelligence (AI), as many types of input data, such as language, video, etc., naturally form sequences and the vast majority is unlabeled.

This thesis will cover four main topics:
\begin{itemize}
	\item Chapter 3 provides an overview of deep architectures - a background on representation learning from probabilistic and direct encoding viewpoints. Recent work on generative viewpoints will be discussed as well, showing how denoising auto-encoders can solve the multi-modal problem via learning a Markov chain transition operator.
	\item Chapter 4 introduces Generative Stochastic Networks - recent work generalizing the denoising auto-encoder framework into GSNs will be explained, as well as how this can be extended to sequence prediction tasks.
	\item Chapters 5, 6, and 7 describe models using GSNs to learn complex sequential input data.
	\item Chapter 8 discusses the results of these three models and baselines and why they are able to use deep representations to learn sequence data.
\end{itemize}