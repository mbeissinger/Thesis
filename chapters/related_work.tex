Due to the success of deep architectures on highly complex input data, applying deep architectures to sequence prediction tasks has been studied extensively in literature. RBM variants have been the most popular for applying deep learning models to sequential data.

\emph{Temporal RBM (TRBM)} \cite{sutskever06} is one of the first frameworks of non-linear sequence models that are more powerful than traditional Hidden Markov models or linear systems. It learns multilevel representations of sequential data by adding connections from previous states of the hidden and visible units to the current states. When the RBM is known, the TRBM learns the dynamic biases of the parameters from one set of states to the next. However, inference over variables is still exponentially difficult.


\emph{Recurrent Temporal RBMs (RTRBMs)} \cite{sutskever08} are an extension of the TRBM. They add a secondary learned latent variable \(H'\) that serves to reduce the number of posterior probabilities needed to consider during inference through a learned generative process. Exact inference can be done easily and gradient learning becomes almost tractable.


\emph{Temporal Convolution Machines (TCMs)} \cite{lockett09} also build from TRBMs. They make better use of prior states by allowing the time-varying bias of the underlying RBM to be a convolution of prior states with any function. Therefore, the states of the TCM can directly depend on arbitrarily distant past states. This means the complexity of the hidden states are reduced, as a complex Markov sequence in the hidden layer is not necessary. However, inference is still difficult.


\emph{RNN-RBM} \cite{lewandowski12} is similar to the RTRBM. The RNN-RBM adds a recursive neural network layer that acts as a dynamic state variable \(u\) which is dependent on the current input data and the past state variable. This state variable is what then determines the bias parameters of the next RBM in the sequence, rather than just a regression from the latents \(H\).


\emph{Sequential Deep Belief Networks (SDBNs)} \cite{andrew12, andrew13}  is a series of stacked RBMs that have a Markov interaction over time between each corresponding hidden layer. Rather than adjusting the bias parameters dynamically like TRBMs, this approach learns a Markov transition between the hidden latent variables over time. This allows the hidden layer to model any dependencies between time frames of the observations.


\emph{Recursive Neural Networks (RNNs)} \cite{socher11} are a slightly different framework used for sequence labeling in parsing natural language sentences or parsing natural scene images that have recursive structures. RNNs define a neural network that takes two possible input vectors (such as adjoining words in a sentence) and produces a hidden representation vector as well as a prediction score of the representation being the correct merging of the two inputs. These hidden representation vectors can be fed recursively into the RNN to calculate the highest probability recursive structure of the input sequence. RNNs are therefore a supervised algorithm.

Past work has also compared a deep architecture, Sentence-level Likelihood Neural Nets (SLNN), with traditional Conditional Random Fields (CRF) for sequence labeling tasks of Named Entity Recognition and Syntactic chunking \cite{wang13}. Wang et al. found that non-linear deep architectures, compared to linear CRFs, are more effective in low dimensional continuous input spaces, but not in high-dimensional discrete input spaces. They also confirm that distributional representations can be used to achieve better generalization.

While many of these related works perform well on sequential data such as video and language, all of them still struggle with inference due to the nature of RBMs. Using these sequential techniques on GSNs, which are easy to sample from and perform inference, have not yet been studied.