\relax 
\citation{bengio_workshop}
\citation{lenet5}
\citation{seide11}
\citation{krizhevsky12}
\citation{hinton06}
\citation{alain12}
\citation{ranzato07}
\citation{gsn}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{sutskever06}
\citation{sutskever08}
\citation{lockett09}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{lewandowski12}
\citation{andrew12}
\citation{andrew13}
\citation{socher11}
\citation{wang13}
\citation{bengio12}
\citation{bengio13}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Background: Deep Architectures}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Representation Learning}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An example deep architecture.}}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Interpretations of Deep Architectures}{7}}
\citation{bach05}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Principal component analysis.}}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Probabilistic models: restricted boltzmann machine (RBM)}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A Restricted Boltzmann machine.}}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Stacked RBM.}}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Direct encoding models: auto-encoder}{11}}
\citation{bengio13a}
\citation{vincent08}
\citation{alain12}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Stacked auto-encoder.}}{12}}
\citation{alain12}
\citation{bengio13a}
\citation{bengio13a}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Denoising Auto-encoders}{13}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces  Generalized Denoising Auto-encoder Training Algorithm }}{13}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces  Walkback Training Algorithm for Denoising Auto-encoders }}{14}}
\citation{bengio13a}
\citation{bengio13a}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}General Methodology: Deep Generative Stochastic Networks (GSN)}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Generalizing denoising auto-encoders}{15}}
\citation{gsn}
\citation{gsn}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Easing restrictive conditions on the denoising auto-encoder}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Generalizing to GSN}{16}}
\citation{rezende14}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces GSN computational graph.}}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Unrolled GSN Markov chain.}}{17}}
\citation{bengio_workshop}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Extension to recurrent deep GSN}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Better mixing via deep architectures \cite  {bengio_workshop}.}}{18}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Model 1: Expectation Maximization method for training a recurrent deep GSN}{20}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Recurrent nature of deep GSNs}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Algorithm}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Experimental results}{20}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Model 2: Online method for training a recurrent deep GSN}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Using real data sequence to train deep GSN}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Algorithm}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Extending the walkback procedure to sequenced inputs}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Analysis}{21}}
\citation{lenet5}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Model 3: Hybrid method for training a recurrent deep GSN}{22}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Generalizing the EM model}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Algorithm}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Experimental results}{22}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Discussion}{23}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{reference/bibliography}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Conclusion}{24}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{bach05}{1}
\bibcite{bengio13}{2}
\bibcite{bengio12}{3}
