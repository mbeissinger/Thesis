\relax 
\citation{bengio_workshop}
\citation{lenet5}
\citation{seide11}
\citation{krizhevsky12}
\citation{hinton06}
\citation{alain12}
\citation{ranzato07}
\citation{gsn}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{sutskever06}
\citation{sutskever08}
\citation{lockett09}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{lewandowski12}
\citation{andrew12}
\citation{andrew13}
\citation{socher11}
\citation{wang13}
\citation{bengio12}
\citation{bengio13}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Background: Deep Architectures}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Representation Learning}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An example deep architecture.}}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Interpretations of Deep Architectures}{7}}
\citation{bach05}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces PCA}}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Probabilistic models: Restricted Boltzmann Machine (RBM)}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A Restricted Boltzmann Machine.}}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Stacked RBM.}}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Direct encoding models: auto-encoder}{11}}
\citation{bengio13a}
\citation{vincent08}
\citation{alain12}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Stacked auto-encoder.}}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Denoising Auto-encoders}{12}}
\citation{alain12}
\citation{bengio13a}
\citation{bengio13a}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces  Generalized Denoising Auto-encoder Training Algorithm }}{13}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces  Walkback Training Algorithm for Denoising Auto-encoders }}{14}}
\citation{bengio13a}
\citation{bengio13a}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}General Methodology: Deep Generative Stochastic Networks (GSN)}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Generalizing denoising auto-encoders}{15}}
\citation{gsn}
\citation{gsn}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Easing restrictive conditions on the denoising auto-encoder}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Generalizing to GSN}{16}}
\citation{rezende14}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces GSN computational graph.}}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Unrolled GSN Markov chain.}}{17}}
\citation{bengio_workshop}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Extension to recurrent deep GSN}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Better mixing via deep architectures \cite  {bengio_workshop}.}}{18}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Model 1: Temporal GSN (TGSN)}{20}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Recurrent nature of deep GSNs}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Samples from GSN after 290 training epochs. Good mixing between major modes in the input space.}}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Algorithm}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Temporal GSN architecture.}}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Experimental setup}{22}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces  Model 1 EM Algorithm }}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Model 1 reconstruction of digits and predicted next digits after 300 iterations.}}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Average MNIST training data by digit.}}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Model 1 sampling after 90 training iterations; smooth mixing between major modes.}}{25}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Model 2: Recurrent GSN (RNN-GSN)}{26}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Aside: GSN as a recurrent network}{26}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces  Untied GSN as an RNN }}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Untied GSN on sequenced MNIST}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Untied GSN reconstruction of predicted next digits and predicted digits 3 iterations ahead after 300 iterations.}}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Extending the walkback procedure to sequenced inputs}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Untied GSN sampling after 300 iterations.}}{29}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces  Walkbacks for sequential input }}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}RNN-GSN: Generalizing the EM training model for the TGSN}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Recurrent GSN architecture. H is GSN hiddens, V is RNN hiddens.}}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Algorithm}{31}}
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces  Recurrent GSN Algorithm }}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Experimental setup}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces RNN-GSN reconstruction of current digits and predicted next digits after 300 iterations.}}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces RNN-GSN sampling after 300 iterations.}}{32}}
\citation{lenet5}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Model 3: Sequence Encoder Network (SEN)}{33}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Algorithm}{33}}
\citation{mathis-mozer}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Sequence Encoder Network architecture. H is GSN hiddens, V is RNN hiddens.}}{34}}
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces  SEN Algorithm }}{35}}
\citation{lewandowski12}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Discussion of Results}{36}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Samples from RNN-RBM on sequenced MNIST}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Bouncing balls videos dataset}{36}}
\citation{sutskever08}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces RNN-RBM sampling after 300 iterations.}}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}CMU motion capture dataset}{38}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Mean squared prediction error on bouncing balls videos and motion capture data. RTRBM and RNN-RBM numbers from [10]}}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Results}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces RNN-GSN good state.}}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces RNN-GSN diverged bad state.}}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces SEN frame predictions after 140 epochs.}}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Future Work}{40}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Conclusion}{42}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{reference/bibliography}
\bibcite{alain12}{1}
\bibcite{andrew12}{2}
\bibcite{andrew13}{3}
\bibcite{bach05}{4}
\bibcite{bengio13}{5}
\bibcite{bengio_workshop}{6}
\bibcite{bengio12}{7}
\bibcite{gsn}{8}
\bibcite{bengio13a}{9}
\bibcite{lewandowski12}{10}
\bibcite{hinton06}{11}
\bibcite{krizhevsky12}{12}
\bibcite{lenet5}{13}
\bibcite{lockett09}{14}
\bibcite{mathis-mozer}{15}
\bibcite{ranzato07}{16}
\bibcite{rezende14}{17}
\bibcite{seide11}{18}
\bibcite{socher11}{19}
\bibcite{sutskever06}{20}
\bibcite{sutskever08}{21}
\bibcite{vincent08}{22}
\bibcite{wang13}{23}
