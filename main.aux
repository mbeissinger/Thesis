\relax 
\citation{bengio_workshop}
\citation{lenet5}
\citation{seide11}
\citation{krizhevsky12}
\citation{hinton06}
\citation{alain12}
\citation{ranzato07}
\citation{gsn}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{sutskever06}
\citation{sutskever08}
\citation{lockett09}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{lewandowski12}
\citation{andrew12}
\citation{andrew13}
\citation{socher11}
\citation{wang13}
\citation{bengio12}
\citation{bengio13}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Background: Deep Architectures}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Representation Learning}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An example deep architecture.}}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Interpretations of Deep Architectures}{7}}
\citation{bach05}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Principal component analysis.}}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Probabilistic models: restricted boltzmann machine (RBM)}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A Restricted Boltzmann machine.}}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Stacked RBM.}}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Direct encoding models: auto-encoder}{11}}
\citation{bengio13a}
\citation{vincent08}
\citation{alain12}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Stacked auto-encoder.}}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Denoising Auto-encoders}{12}}
\citation{alain12}
\citation{bengio13a}
\citation{bengio13a}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces  Generalized Denoising Auto-encoder Training Algorithm }}{13}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces  Walkback Training Algorithm for Denoising Auto-encoders }}{14}}
\citation{bengio13a}
\citation{bengio13a}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}General Methodology: Deep Generative Stochastic Networks (GSN)}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Generalizing denoising auto-encoders}{15}}
\citation{gsn}
\citation{gsn}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Easing restrictive conditions on the denoising auto-encoder}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Generalizing to GSN}{16}}
\citation{rezende14}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces GSN computational graph.}}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Unrolled GSN Markov chain.}}{17}}
\citation{bengio_workshop}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Extension to recurrent deep GSN}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Better mixing via deep architectures \cite  {bengio_workshop}.}}{18}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Model 1: Expectation Maximization method for training a recurrent deep GSN}{20}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Recurrent nature of deep GSNs}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Samples from GSN after 290 training epochs. Good mixing between major modes in the input space.}}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Algorithm}{22}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces  Model 1 EM Algorithm }}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Experimental results}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Model 1 reconstruction of digits and predicted next digits after 300 iterations.}}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Average MNIST training data by digit.}}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Model 1 sampling after 90 training iterations.}}{24}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Model 2: Online method for training a recurrent deep GSN}{25}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Online learning with real-sequenced data }{25}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Algorithm}{26}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces  Model 2 Online Algorithm }}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Results}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Model 2 reconstruction of predicted next digits and predicted digits 3 iterations ahead after 300 iterations.}}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Extending the walkback procedure to sequenced inputs}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Model 2 sampling after 300 iterations.}}{28}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces  Walkbacks for sequential input }}{29}}
\citation{lenet5}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Model 3: Hybrid method for training a recurrent deep GSN}{30}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Generalizing the EM model}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Algorithm}{31}}
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces  Model 3 Hybrid Recurrent Deep GSN Algorithm }}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Experimental results}{31}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Discussion of Results}{32}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Binary cross-entropy of the predicted sequence reconstruction comparison of the 3 models and an RNN-RBM on artificially sequenced (0-9 repeating) MNIST data.}}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces RNN-RBM sampling after 300 iterations.}}{33}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Conclusion}{34}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{reference/bibliography}
\bibcite{alain12}{1}
\bibcite{andrew12}{2}
\bibcite{andrew13}{3}
\bibcite{bach05}{4}
\bibcite{bengio13}{5}
\bibcite{bengio_workshop}{6}
\bibcite{bengio12}{7}
\bibcite{gsn}{8}
\bibcite{bengio13a}{9}
\bibcite{lewandowski12}{10}
\bibcite{hinton06}{11}
\bibcite{krizhevsky12}{12}
\bibcite{lenet5}{13}
\bibcite{lockett09}{14}
\bibcite{ranzato07}{15}
\bibcite{rezende14}{16}
\bibcite{seide11}{17}
\bibcite{socher11}{18}
\bibcite{sutskever06}{19}
\bibcite{sutskever08}{20}
\bibcite{vincent08}{21}
\bibcite{wang13}{22}
