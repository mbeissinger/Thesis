\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{bengio12}
\citation{bengio13}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Background}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Representation Learning}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An example deep architecture.}}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Interpretations of Deep Architectures}{4}}
\citation{bach05}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Principal component analysis.}}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Probabilistic models: restricted boltzmann machine (RBM)}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A graphical representation of a Boltzmann machine. Each undirected edge represents dependency; in this example there are 3 hidden units and 4 visible units.}}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A Restricted Boltzmann machine.}}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Stacked RBM.}}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Direct encoding models: auto-encoder}{9}}
\citation{bengio13a}
\citation{vincent08}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Stacked auto-encoder.}}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Denoising Auto-encoders}{10}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces  Generalized Denoising Auto-encoder Training Algorithm }}{10}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}General Methodology: Deep Generative Stochastic Networks (GSN)}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Algorithm}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Analysis}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Extension to recurrent deep GSN}{11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Model 1: Expectation Maximization method for training a recurrent deep GSN}{12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Assumptions}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Algorithm}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Analysis}{12}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Model 2: Online method for training a recurrent deep GSN}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Assumptions}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Algorithm}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Analysis}{13}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Discussion}{14}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{reference/bibliography}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusion}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{bach05}{1}
\bibcite{bengio13}{2}
\bibcite{bengio12}{3}
